{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# üöÄ STEP 1: Install Required Packages\n",
        "!pip install -q transformers accelerate\n",
        "\n",
        "# üöÄ STEP 2: Import Dependencies\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch\n",
        "import pandas as pd\n",
        "from IPython.display import Markdown, display\n",
        "import time\n",
        "import re\n",
        "\n",
        "# üöÄ STEP 3: Load Fast, Free LLM (No Key Required)\n",
        "model_id = \"tiiuae/falcon-rw-1b\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "chat = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=150, do_sample=False)\n",
        "\n",
        "# üöÄ STEP 4: Role-specific Topics\n",
        "roles = {\n",
        "    \"ML Engineer\": [\"model overfitting\", \"regularization\", \"gradient descent\", \"bias-variance\"],\n",
        "    \"Frontend Developer\": [\"React lifecycle\", \"DOM\", \"CSS vs SCSS\", \"event bubbling\"],\n",
        "    \"Data Analyst\": [\"SQL joins\", \"pandas\", \"data cleaning\", \"pivot tables\"],\n",
        "    \"AI Research Intern\": [\n",
        "    \"transformers\", \"fine-tuning\", \"language models\", \"attention mechanism\",\n",
        "    \"gradient descent\", \"regularization\", \"autoencoders\", \"GANs\", \"evaluation metrics\",\n",
        "    \"BERT vs GPT\", \"tokenization\", \"loss functions\", \"bias-variance tradeoff\"\n",
        "]\n",
        "\n",
        "}\n",
        "import random\n",
        "\n",
        "previous_questions = set()  # To store and check duplicates\n",
        "\n",
        "history = []\n",
        "\n",
        "def ask_question(role, mode):\n",
        "    topics = \", \".join(roles.get(role, []))\n",
        "\n",
        "    variations = [\n",
        "        f\"You are conducting a {mode} interview for the role of {role}. Ask a unique question related to: {topics}.\",\n",
        "        f\"As a recruiter for {role}, ask a new {mode} interview question. Topics: {topics}.\",\n",
        "        f\"Interviewing a candidate for {role}. Give one relevant {mode} question from these topics: {topics}.\",\n",
        "    ]\n",
        "\n",
        "    for _ in range(5):  # Try a few times to get a new question\n",
        "        prompt = random.choice(variations)\n",
        "        result = chat(prompt)[0]['generated_text']\n",
        "        generated = result.replace(prompt, \"\").strip()\n",
        "\n",
        "        # Extract a line with a question mark\n",
        "        lines = generated.split(\"\\n\")\n",
        "        for line in lines:\n",
        "            question = line.strip()\n",
        "            if \"?\" in question and question not in previous_questions:\n",
        "                previous_questions.add(question)\n",
        "                return question\n",
        "\n",
        "    # If nothing new found\n",
        "    return \"Can you explain a key concept related to \" + random.choice(roles.get(role, [])) + \"?\"\n",
        "\n",
        "def evaluate_answer(question, answer):\n",
        "    prompt = (\n",
        "        \"You are a strict but fair technical interviewer. For each Q&A below, give a score out of 10 and short feedback.\\n\\n\"\n",
        "        \"Q: What is overfitting?\\n\"\n",
        "        \"A: When a model memorizes training data and performs poorly on unseen data.\\n\"\n",
        "        \"Feedback: Good explanation with correct concept. Score: 9/10\\n\\n\"\n",
        "        \"Q: What is overfitting?\\n\"\n",
        "        \"A: I don't know.\\n\"\n",
        "        \"Feedback: Incomplete answer. Needs revision. Score: 2/10\\n\\n\"\n",
        "        f\"Q: {question}\\n\"\n",
        "        f\"A: {answer}\\n\"\n",
        "        f\"Feedback:\"\n",
        "    )\n",
        "\n",
        "    result = chat(prompt)[0]['generated_text']\n",
        "    feedback = result.replace(prompt, \"\").strip().split(\"\\n\")[0]\n",
        "    return feedback\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# üöÄ STEP 7: Extract Score from Feedback\n",
        "def extract_score(feedback):\n",
        "    match = re.search(r'([0-9]+)/10', feedback)\n",
        "    if match:\n",
        "        return int(match.group(1))\n",
        "    match = re.search(r'\\b([1-9]|10)\\b', feedback)\n",
        "    return int(match.group(1)) if match else 5\n",
        "\n",
        "# üöÄ STEP 8: Run the Full Interview Loop\n",
        "def run_interview(role=\"AI Research Intern\", rounds=3):\n",
        "    mode = input(\"Select question type (technical / behavioral): \").lower()\n",
        "    total_score = 0\n",
        "\n",
        "    for i in range(rounds):\n",
        "        q = ask_question(role, mode)\n",
        "        display(Markdown(f\"### üßë Interviewer: {q}\"))\n",
        "\n",
        "        print(\"‚è±Ô∏è You have 60 seconds to answer...\")\n",
        "        start = time.time()\n",
        "        a = input(\"üßë‚Äçüíª Your Answer: \")\n",
        "        end = time.time()\n",
        "\n",
        "        if end - start > 60:\n",
        "            print(\"‚ö†Ô∏è Time exceeded!\")\n",
        "\n",
        "        fb = evaluate_answer(q, a)\n",
        "        score = extract_score(fb)\n",
        "        total_score += score\n",
        "\n",
        "        display(Markdown(f\"**üß† Feedback:** {fb}\"))\n",
        "        history.append({\"question\": q, \"answer\": a, \"feedback\": fb, \"score\": score})\n",
        "\n",
        "    avg = total_score / rounds\n",
        "    print(f\"\\n‚úÖ Interview complete! Average Score: {avg:.1f}/10\")\n",
        "\n",
        "# üöÄ STEP 9: Start Interview!\n",
        "run_interview(\"AI Research Intern\", rounds=3)\n",
        "\n",
        "# üöÄ STEP 10: Save Results to CSV\n",
        "df = pd.DataFrame(history)\n",
        "df.to_csv(\"interview_session.csv\", index=False)\n",
        "print(\"üìù Saved as interview_session.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 735
        },
        "id": "odhj05okZZO0",
        "outputId": "d0b1ab14-41b1-43b1-a409-a46fbaaab2be"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Select question type (technical / behavioral): technical\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### üßë Interviewer: - What is the difference between a transformer and a fine-tuner?"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚è±Ô∏è You have 60 seconds to answer...\n",
            "üßë‚Äçüíª Your Answer: dont know\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**üß† Feedback:** I don't know. Score: 1/10"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### üßë Interviewer: - What is the difference between a gradient descent and a regularization?"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚è±Ô∏è You have 60 seconds to answer...\n",
            "üßë‚Äçüíª Your Answer: i am unable to answer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**üß† Feedback:** No feedback. Score: 0/10"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### üßë Interviewer: - What is the difference between a gradient descent and a gradient-based regularization?"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚è±Ô∏è You have 60 seconds to answer...\n",
            "üßë‚Äçüíª Your Answer: ask me different question\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**üß† Feedback:** No feedback. Score: 0/10"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Interview complete! Average Score: 0.3/10\n",
            "üìù Saved as interview_session.csv\n"
          ]
        }
      ]
    }
  ]
}